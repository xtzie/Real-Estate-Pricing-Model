{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:13:59.707412Z",
     "start_time": "2019-11-27T22:13:59.687Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Web\n",
    "\n",
    "So far we've used BeautifulSoup to parse our own HTML strings and files.  Now let's scrape Box Office Mojo.\n",
    "\n",
    "First let's take a look at some source code for _The Big Lebowski_.\n",
    "\n",
    "- Navigate to https://www.boxofficemojo.com/title/tt0118715/ in your browser, preferably Chrome\n",
    "- Right click and select \"Inspect\"\n",
    "- Alternatively, you can \"View Page Source\"\n",
    "\n",
    "To retrieve the HTML for this webpage, we will use `requests`.\n",
    "\n",
    "### `requests`\n",
    "\n",
    "The `requests` library allows us to grab information from the web.  There are two common types of requests:\n",
    "- `get` -- simply requests information, akin to putting a url in your browser\n",
    "- `post` -- sends information to the website, for example, writing an email\n",
    "\n",
    "We will be using `get` to retrieve a page's HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.114605Z",
     "start_time": "2019-11-27T22:13:59.922Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-0794d688ab3c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0794d688ab3c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    url = 'https://www.zillow.com/homes/?searchQueryState={%22pagination%22:{%22currentPage%22:2},%22mapBounds%22:{%22west%22:-122.59102821350098,%22east%22:-122.40254402160645,%22south%22:37.71146108878938,%22north%22:37.84822253175519},%22regionSelection%22:[{%22regionId%22:20330,%22regionType%22:6}],%22isMapVisible%22:true,%22filterState%22:{%22sortSelection%22:{%22value%22:%22globalrelevanceex%22}},%22isListVisible%22:true,%22mapZoom%22:12}\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.zillow.com/homes/?searchQueryState={%22pagination%22:{%22currentPage%22:2},%22mapBounds%22:{%22west%22:-122.59102821350098,%22east%22:-122.40254402160645,%22south%22:37.71146108878938,%22north%22:37.84822253175519},%22regionSelection%22:[{%22regionId%22:20330,%22regionType%22:6}],%22isMapVisible%22:true,%22filterState%22:{%22sortSelection%22:{%22value%22:%22globalrelevanceex%22}},%22isListVisible%22:true,%22mapZoom%22:12} \n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we got back is an object that gives us access to:\n",
    "- `response.text` -- the returned HTML (if any)\n",
    "- `response.json` -- the returned JSON (if any), typical for APIs\n",
    "- `response.status_code` -- a [code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) to tell you if your request was successful or if an error occurred, 2XX indicates success while 404 means not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.118688Z",
     "start_time": "2019-11-27T22:13:59.930Z"
    }
   },
   "outputs": [],
   "source": [
    "response.status_code  #200 = success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.122124Z",
     "start_time": "2019-11-27T22:13:59.933Z"
    }
   },
   "outputs": [],
   "source": [
    "response.text[:1000]  #First 1000 characters of the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.125966Z",
     "start_time": "2019-11-27T22:13:59.935Z"
    }
   },
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BeautifulSoup` Basics\n",
    "\n",
    "Now that we have the HTML, let's learn its structure by parsing with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.133008Z",
     "start_time": "2019-11-27T22:13:59.945Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.137230Z",
     "start_time": "2019-11-27T22:13:59.947Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prettify` method turns the soup into a nicely formatted Unicode string with one tag on each line for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.143509Z",
     "start_time": "2019-11-27T22:13:59.955Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "> Select the first link on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now select the LAST link on the page.  Can you get the text and the URL associated with this link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember `find` gets only one match, but `find_all` retrieves all matches in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.147863Z",
     "start_time": "2019-11-27T22:13:59.986Z"
    }
   },
   "outputs": [],
   "source": [
    "for link in soup.find_all('a')[:5]:\n",
    "    print(link, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can match only those with a specific `id` or `class` if you'd like.  Here are all the elements labeled with the \"mojo-navigtaion-tab\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.151470Z",
     "start_time": "2019-11-27T22:13:59.997Z"
    }
   },
   "outputs": [],
   "source": [
    "for element in soup.find_all(class_='mojo-navigation-tab'):\n",
    "    print(element.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember `find` and `find_all` return BeautifulSoup elements. You can continue searching these elements, thus chaining commands together.\n",
    "\n",
    "Basic earnings information can be found in the `div` with the \"mojo-performance-summary-table\" class.  Let's extract the domestic gross from this element.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/biglebow_table.png\" alt=\"Big Lebowski Table\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.154547Z",
     "start_time": "2019-11-27T22:14:00.007Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.find(class_='mojo-performance-summary-table').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.158500Z",
     "start_time": "2019-11-27T22:14:00.010Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text needs to be extracted from one element at a time.  To get the domestic gross:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.283698Z",
     "start_time": "2019-11-27T22:14:00.266844Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find using an `id`; remember id should be unique to just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.293048Z",
     "start_time": "2019-11-27T22:14:00.039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.find(id='tabs').prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Pipeline\n",
    "\n",
    "Now that we have the basics, let's practice web scraping.  **The main goal of web scraping is to extract data by taking advantage of a site's consistent format.**  That is, the code you write for one page on a website can hopefully be used on multiple pages to gather more information automatically.\n",
    "\n",
    "Let's create code to get the following information for the movies on Box Office Mojo:\n",
    "- Movie title\n",
    "- Domestic gross\n",
    "- Runtime\n",
    "- MPAA rating\n",
    "- Release date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.299455Z",
     "start_time": "2019-11-27T22:14:00.065Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.303669Z",
     "start_time": "2019-11-27T22:14:00.069Z"
    }
   },
   "outputs": [],
   "source": [
    "title_string = soup.find('title').text\n",
    "\n",
    "title_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.306661Z",
     "start_time": "2019-11-27T22:14:00.073Z"
    }
   },
   "outputs": [],
   "source": [
    "title_string.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.308637Z",
     "start_time": "2019-11-27T22:14:00.078Z"
    }
   },
   "outputs": [],
   "source": [
    "title = title_string.split('-')[0].strip()\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domestic Gross: \n",
    "\n",
    "As we saw previously, the domestic gross can be found in a `span` within the \"mojo-performance-summary-table\" `div`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.311354Z",
     "start_time": "2019-11-27T22:14:00.093Z"
    }
   },
   "outputs": [],
   "source": [
    "dtg = soup.find(class_='mojo-performance-summary-table').find_all('span', class_='money')[0].text\n",
    "dtg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the information lives in this neighboring `div`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/biglebow_info.png\" alt=\"Big Lebowski Information\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime: `.findNext()`\n",
    "\n",
    "Sometimes you can find the information you are looking for by using text matching.  But note this must be an exact match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.316173Z",
     "start_time": "2019-11-27T22:14:00.145Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find(text='Run')  #does not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.320001Z",
     "start_time": "2019-11-27T22:14:00.162Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find(text='Running Time')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use [regular expressions](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.324016Z",
     "start_time": "2019-11-27T22:14:00.179Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "runtime_regex = re.compile('Run')\n",
    "soup.find(text=runtime_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.327979Z",
     "start_time": "2019-11-27T22:14:00.182Z"
    }
   },
   "outputs": [],
   "source": [
    "rt_string = soup.find(text=re.compile('Run'))\n",
    "print(rt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.332583Z",
     "start_time": "2019-11-27T22:14:00.188Z"
    }
   },
   "outputs": [],
   "source": [
    "type(rt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string we found is still a Beautiful Soup element. This means we can use it to navigate to the next element in the HTML, which is a `span` containing the actual runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.338179Z",
     "start_time": "2019-11-27T22:14:00.206Z"
    }
   },
   "outputs": [],
   "source": [
    "rt_string.findNext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.findNext()` method can be incredibly useful when the information you want to find doesn't have a obvious tag, class, id, etc.\n",
    "\n",
    "Let's clean this value up into usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.341525Z",
     "start_time": "2019-11-27T22:14:00.218Z"
    }
   },
   "outputs": [],
   "source": [
    "rt = rt_string.findNext().text\n",
    "rt = rt.split()\n",
    "minutes = int(rt[0])*60 + int(rt[2])\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPAA Rating, Release Date\n",
    "\n",
    "_**STEP 1:** Create function to grab values_ \n",
    "\n",
    "The text matching method can also help us get runtime, rating, and release date, so let's make a reuable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.345291Z",
     "start_time": "2019-11-27T22:14:00.232Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Box Office Mojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and returns the string in\n",
    "    the next sibling object (the value for that attribute) or None if nothing is found.\n",
    "    '''\n",
    "    \n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    \n",
    "    if not obj: \n",
    "        return None\n",
    "    \n",
    "    # this works for most of the values\n",
    "    next_element = obj.findNext()\n",
    "    \n",
    "    if next_element:\n",
    "        return next_element.text \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.350096Z",
     "start_time": "2019-11-27T22:14:00.238Z"
    }
   },
   "outputs": [],
   "source": [
    "# runtime\n",
    "runtime = get_movie_value(soup,'Run')\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.354712Z",
     "start_time": "2019-11-27T22:14:00.241Z"
    }
   },
   "outputs": [],
   "source": [
    "# rating\n",
    "rating = get_movie_value(soup,'MPAA')\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.359269Z",
     "start_time": "2019-11-27T22:14:00.247Z"
    }
   },
   "outputs": [],
   "source": [
    "release_date = get_movie_value(soup,'Release Date')\n",
    "print(release_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.364377Z",
     "start_time": "2019-11-27T22:14:00.252Z"
    }
   },
   "outputs": [],
   "source": [
    "release_date = release_date.split('\\n')[0]  #Select the only the date\n",
    "print(release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**STEP 2:** Create helper functions to parse strings into appropriate data types_\n",
    "\n",
    "The returned values all need a bit of formatting before we can work with this data.  Here are a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.368832Z",
     "start_time": "2019-11-27T22:14:00.265Z"
    }
   },
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "    return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_date(datestring):\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**STEP 3:** Apply these conversions_\n",
    "\n",
    "Let's get these values again and format them all in one swoop. (Note: Rating is already correct as a string.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.494218Z",
     "start_time": "2019-11-27T22:14:00.479650Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_domestic_total_gross = dtg\n",
    "domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "raw_runtime = get_movie_value(soup,'Running')\n",
    "runtime = runtime_to_minutes(raw_runtime)\n",
    "\n",
    "raw_release_date = get_movie_value(soup,'Release Date').split('\\n')[0]\n",
    "release_date = to_date(raw_release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Results in Dictionary\n",
    "\n",
    "Now that we have results for all five quantities, we can store them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.497551Z",
     "start_time": "2019-11-27T22:14:00.305Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = ['movie title', 'domestic total gross',\n",
    "           'runtime (mins)', 'rating', 'release date']\n",
    "\n",
    "movie_data = []\n",
    "movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                runtime,\n",
    "                                rating, \n",
    "                                release_date]))\n",
    "\n",
    "movie_data.append(movie_dict)\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "> Why might we want to store these data in a dictionary?  Why did we put the dictionary in a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Tables\n",
    "\n",
    "Let's take a look at the [top G-rated movies](https://www.boxofficemojo.com/chart/mpaa_title_lifetime_gross/?by_mpaa=G) of Box Office Mojo.  How could we pull all the data from this main page?\n",
    "\n",
    "First request the HTML and parse it with Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.500634Z",
     "start_time": "2019-11-27T22:14:00.326Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/chart/mpaa_title_lifetime_gross/?by_mpaa=G'\n",
    "\n",
    "response = requests.get(url)\n",
    "page = response.text\n",
    "\n",
    "soup = BeautifulSoup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the main table; its the only `table` on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.504625Z",
     "start_time": "2019-11-27T22:14:00.339Z"
    }
   },
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.507480Z",
     "start_time": "2019-11-27T22:14:00.343Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = [row for row in table.find_all('tr')]  # tr tag is for rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row contains the information we want but requires more parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.511185Z",
     "start_time": "2019-11-27T22:14:00.353Z"
    }
   },
   "outputs": [],
   "source": [
    "rows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: you can chain methods together to look for information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.513681Z",
     "start_time": "2019-11-27T22:14:00.363Z"
    }
   },
   "outputs": [],
   "source": [
    "rows[1].find_all('td')[0].find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab data for the first 5 movies with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T22:14:00.515464Z",
     "start_time": "2019-11-27T22:14:00.373Z"
    }
   },
   "outputs": [],
   "source": [
    "movies = {}\n",
    "\n",
    "for row in rows[1:6]:\n",
    "    items = row.find_all('td')\n",
    "    link = items[0].find('a')\n",
    "    title, url = link.text, link['href']\n",
    "    movies[title] = [url] + [i.text for i in items]\n",
    "    \n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- Beautiful Soup is a powerful HTML parser\n",
    "- You can locate one element with `.find()` or all matching elements with `.find_all()`\n",
    "- To select specific elements, you can filter by tags like `class` or `id` \n",
    "- You can also find items using text matching and `.findNext()`, `.findNextSibling()`, `.findChild()`, etc.\n",
    "\n",
    "### Limitations\n",
    "Beautiful Soup has its limitations though.  For example, we can't use Beautiful Soup if a page:\n",
    "- Requires us to input a password\n",
    "- Reveals information we want only when we interact with it\n",
    "- Generates dynamically (with JavaScript) rather than statically serving HTML\n",
    "\n",
    "For these situations we need a different tool, like **Selenium** -- coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
